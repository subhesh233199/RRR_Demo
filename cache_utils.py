"""
This module provides caching functionality for analysis results using SQLite.
It implements a thread-safe caching system that stores and retrieves analysis
results based on folder path and PDF content hashes.

The cache has a TTL (Time To Live) of 3 days, after which entries are automatically
cleaned up. The module uses thread locks to ensure thread safety during cache
operations.

Key features:
- SQLite-based persistent storage
- Thread-safe operations
- Automatic cache cleanup
- Hash-based cache keys
- JSON serialization of results
"""

import os
import re
import json
import runpy
import base64
import sqlite3
import hashlib
import time
from typing import List, Dict, Tuple, Any, Union
from threading import Lock
from concurrent.futures import ThreadPoolExecutor, as_completed
import asyncio
from dotenv import load_dotenv
from PyPDF2 import PdfReader
from crewai import Agent, Task, Crew, Process, LLM
from langchain_openai import AzureChatOpenAI
import ssl
import warnings
import shutil
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, validator
import logging
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from tenacity import retry, stop_after_attempt, wait_fixed
from copy import deepcopy
import pdfplumber
from crewai import Agent, Task, Crew, Process 
from models import AnalysisResponse
from shared_state import shared_state
from app_logging import logger

# Cache TTL in seconds (3 days)
CACHE_TTL_SECONDS = 3 * 24 * 60 * 60

def init_cache_db():
    """
    Initializes SQLite database for caching analysis results.
    
    Creates a new SQLite database file 'cache.db' if it doesn't exist,
    with a table 'report_cache' that stores:
    - folder_path_hash: Hash of the folder path (PRIMARY KEY)
    - pdfs_hash: Hash of PDF contents
    - report_json: JSON string of the analysis results
    - created_at: Timestamp of cache entry creation
    
    The database uses WAL (Write-Ahead Logging) mode for better concurrency.
    """
    conn = sqlite3.connect('cache.db')
    cursor = conn.cursor()
    cursor.execute("PRAGMA journal_mode=WAL")  # Enable WAL for better concurrency
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS report_cache (
            folder_path_hash TEXT PRIMARY KEY,
            pdfs_hash TEXT NOT NULL,
            report_json TEXT NOT NULL,
            created_at INTEGER NOT NULL
        )
    ''')
    conn.commit()
    conn.close()

def hash_string(s: str) -> str:
    """
    Generates an MD5 hash of a string.
    
    Args:
        s (str): The string to hash
        
    Returns:
        str: MD5 hash of the input string
    """
    return hashlib.md5(s.encode('utf-8')).hexdigest()

def hash_pdf_contents(pdf_files: List[str]) -> str:
    """
    Generates a combined MD5 hash of multiple PDF files.
    
    The hash is generated by reading each PDF file in chunks and updating
    the hash with each chunk. Files are processed in sorted order to ensure
    consistent hashing regardless of input order.
    
    Args:
        pdf_files (List[str]): List of paths to PDF files
        
    Returns:
        str: MD5 hash of all PDF contents combined
        
    Raises:
        Exception: If any PDF file cannot be read
    """
    hasher = hashlib.md5()
    for pdf_path in sorted(pdf_files):
        try:
            with open(pdf_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hasher.update(chunk)
        except Exception as e:
            logger.error(f"Error hashing PDF {pdf_path}: {str(e)}")
            raise
    return hasher.hexdigest()

def get_cached_report(folder_path_hash: str, pdfs_hash: str) -> Union[AnalysisResponse, None]:
    """
    Retrieves cached analysis results if available and not expired.
    
    Checks the cache for a matching entry based on folder path hash and PDFs hash.
    If found and not expired (within TTL), returns the cached results.
    If expired, deletes the entry from cache.
    
    Args:
        folder_path_hash (str): Hash of folder path
        pdfs_hash (str): Hash of PDF contents
        
    Returns:
        Union[AnalysisResponse, None]: Cached results if found and valid,
            None otherwise
    """
    try:
        conn = sqlite3.connect('cache.db')
        cursor = conn.cursor()
        cursor.execute('''
            SELECT report_json, created_at
            FROM report_cache
            WHERE folder_path_hash = ? AND pdfs_hash = ?
        ''', (folder_path_hash, pdfs_hash))
        result = cursor.fetchone()
        conn.close()

        if result:
            report_json, created_at = result
            current_time = int(time.time())
            if current_time - created_at < CACHE_TTL_SECONDS:
                report_dict = json.loads(report_json)
                return AnalysisResponse(**report_dict)
            else:
                with shared_state.lock:
                    conn = sqlite3.connect('cache.db')
                    cursor = conn.cursor()
                    cursor.execute('DELETE FROM report_cache WHERE folder_path_hash = ?', (folder_path_hash,))
                    conn.commit()
                    conn.close()
        return None
    except Exception as e:
        logger.error(f"Error retrieving cached report: {str(e)}")
        return None
    
def store_cached_report(folder_path_hash: str, pdfs_hash: str, response: AnalysisResponse):
    """
    Stores analysis results in cache.
    
    Saves the analysis results to the cache database with the current timestamp.
    Uses thread lock to ensure thread safety during write operations.
    
    Args:
        folder_path_hash (str): Hash of folder path
        pdfs_hash (str): Hash of PDF contents
        response (AnalysisResponse): Analysis results to cache
    """
    try:
        report_json = json.dumps(response.dict())
        current_time = int(time.time())
        with shared_state.lock:
            conn = sqlite3.connect('cache.db')
            cursor = conn.cursor()
            cursor.execute('''
                INSERT OR REPLACE INTO report_cache (folder_path_hash, pdfs_hash, report_json, created_at)
                VALUES (?, ?, ?, ?)
            ''', (folder_path_hash, pdfs_hash, report_json, current_time))
            conn.commit()
            conn.close()
        logger.info(f"Cached report for folder_path_hash: {folder_path_hash}")
    except Exception as e:
        logger.error(f"Error storing cached report: {str(e)}")

def cleanup_old_cache():
    """
    Removes expired cache entries.
    
    Deletes all cache entries that are older than CACHE_TTL_SECONDS.
    Uses thread lock to ensure thread safety during cleanup.
    
    Returns:
        None
    """
    try:
        current_time = int(time.time())
        with shared_state.lock:
            conn = sqlite3.connect('cache.db')
            cursor = conn.cursor()
            cursor.execute('''
                DELETE FROM report_cache
                WHERE created_at < ?
            ''', (current_time - CACHE_TTL_SECONDS,))
            deleted_rows = cursor.rowcount
            conn.commit()
            conn.close()
        logger.info(f"Cleaned up old cache entries, deleted {deleted_rows} rows")
    except Exception as e:
        logger.error(f"Error cleaning up old cache entries: {str(e)}")